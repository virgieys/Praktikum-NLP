{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Virgie Yunita Salsabil\n",
        "\n",
        "NIM : 21110022\n",
        "\n",
        "Kelas : S1-SD02A"
      ],
      "metadata": {
        "id": "Gfe4Hz24vDKq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi4s-oi6mGw1"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Library untuk operasi input/output\n",
        "import io\n",
        "# Library untuk menghasilkan bilangan acak atau pemilihan elemen acak\n",
        "import random\n",
        "# Library untuk manipulasi string\n",
        "import string\n",
        "# Library untuk memberikan peringatan\n",
        "import warnings\n",
        "# Library untuk operasi array numerik dan manipulasi data\n",
        "import numpy as np\n",
        "# Library untuk pemrosesan teks menggunakan TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Library untuk mengukur kemiripan kosinus antara dua set dokumen\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Menyaring peringatan agar tidak ditampilkan\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "rUyRKyH3vigX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cA1u6tEmGxP",
        "outputId": "4f7aa4cb-67c1-489f-8281-e25cc96ffbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "# Instalasi library Natural Language Toolkit (nltk) menggunakan pip\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ObYcoevmGxR",
        "outputId": "1887553e-21fd-46f8-a3ff-847468a1b516"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Impor library NLTK\n",
        "import nltk\n",
        "# Import library untuk lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Pengunduhan data popular dari NLTK\n",
        "nltk.download('popular', quiet=True)\n",
        "#nltk.download('punkt') # first-time use only\n",
        "#nltk.download('wordnet') # first-time use only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW0vRpdxmGxV"
      },
      "source": [
        "## Reading in the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7APugjQ-mGxV"
      },
      "source": [
        "Program ini menggunakan halaman Wikipedia sebagai corpus dari chatpotdengan cara meng-Copy konten halaman dan menyimpannya dalam\n",
        "file ‘chatbot.txtʼ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuka file 'chatbot.txt' untuk dibaca ('r')\n",
        "# Parameter 'errors='ignore'' digunakan untuk mengabaikan karakter yang tidak dapat di-decode\n",
        "f = open('chatbot.txt', 'r', errors='ignore')\n",
        "\n",
        "# Membaca seluruh isi file dan menyimpannya dalam variabel 'raw'\n",
        "# raw kini berisi semua data dari corpus per baris (raw)\n",
        "raw = f.read()\n",
        "\n",
        "# Mengonversi semua teks dalam 'raw' menjadi huruf kecil (lowercase)\n",
        "raw = raw.lower()"
      ],
      "metadata": {
        "id": "Yst9RvAnENe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kDEiVGrmGxY"
      },
      "source": [
        "## Tokenisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFBPF1-cmGxZ"
      },
      "outputs": [],
      "source": [
        "# Tokenisasi adalah memilah-milah dokumen ke kalimat-kalimat,\n",
        "# kemudian memilah setiap kalimat menjadi sekumpulan kata kata\n",
        "sent_tokens = nltk.sent_tokenize(raw) # converts dokumen corpus ke kalimat-kalimat\n",
        "word_tokens = nltk.word_tokenize(raw) # converts dokumen corpus ke kata-kata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWYe3_YbmGxb"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat objek lemmatizer menggunakan WordNetLemmatizer dari NLTK\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# WordNet adalah kamus berorientasi semantik Bahasa Inggris yang disertakan dalam NLTK.\n",
        "# Fungsi LemTokens(tokens) untuk melakukan lemmatization pada setiap token dalam list 'tokens'\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Membuat kamus yang akan digunakan untuk menghapus tanda baca dari teks\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "# Fungsi LemNormalize(text) untuk melakukan normalisasi teks dengan tokenisasi, lemmatization, dan konversi huruf kecil\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "grKvUHU6Kc82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNFcmoR2mGxd"
      },
      "source": [
        "## Keyword matching"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar kata-kata pembuka yang mungkin diterima sebagai salam\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\", \"hai\")\n",
        "\n",
        "# Respon-respon yang mungkin diberikan sebagai tanggapan terhadap salam\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "# Fungsi greeting(sentence) untuk menentukan respon terhadap kalimat yang mengandung kata-kata pembuka\n",
        "def greeting(sentence):\n",
        "    # Melakukan iterasi pada setiap kata dalam kalimat\n",
        "    for word in sentence.split():\n",
        "        # Memeriksa apakah kata dalam bentuk huruf kecil terdapat dalam daftar kata-kata pembuka\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            # Jika ditemukan kata pembuka, maka pilih secara acak salah satu respon dari daftar respon\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "metadata": {
        "id": "iHORgge8KsYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPZiyzaHmGxf"
      },
      "source": [
        "## Generating Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyZSTDSkmGxf"
      },
      "source": [
        "After the initial preprocessing phase, we need to transform text into a meaningful vector (or array) of numbers. The bag-of-words is a\n",
        "representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "- A vocabulary of known words.\n",
        "- A measure of the presence of known words.\n",
        "\n",
        "\n",
        "Why is it is called a “bag” of words? That is because any information about the order or structure of words in the document is discarded and the\n",
        "model is only concerned with whether the known words occur in the document, not where they occur in the document.\n",
        "The intuition behind the Bag of Words is that documents are similar if they have similar content. Also, we can learn something about the\n",
        "meaning of the document from its content alone.\n",
        "For example, if our dictionary contains the words {Learning, is, the, not, great}, and we want to vectorize the text “Learning is great”, we would\n",
        "have the following vector: (1, 1, 0, 0, 1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYzemwWKmGxh"
      },
      "source": [
        "## TF-IDF Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TmPZYSZmGxh"
      },
      "source": [
        "A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not\n",
        "contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents. One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the”\n",
        "that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or\n",
        "TF-IDF for short, where:\n",
        "\n",
        "Term Frequency: is a scoring of the frequency of the word in the current document.\n",
        "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
        "\n",
        "Inverse Document Frequency: is a scoring of how rare the word is across documents.\n",
        "- IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5fB5qdEmGxj"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf8hHamumGxk"
      },
      "source": [
        "Tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important\n",
        "a word is to a document in a collection or corpus\n",
        "\n",
        "- Cosine Similarity (d1, d2) = Dot product(d1, d2) / ||d1|| * ||d2||\n",
        "\n",
        "where d1,d2 are two non zero vectors.\n",
        "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which\n",
        "searches the userʼs utterance for one or more known keywords and returns one of several possible responses. If it doesnʼt find the input\n",
        "matching any of the keywords, it returns a response:” I am sorry! I donʼt understand you”\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menetapkan respon jawaban\n",
        "def response(user_response):\n",
        "    robo_response = ''  # Pada tahap awal, respon mesin diisi karakter kosong\n",
        "    sent_tokens.append(user_response)  # Pertanyaan user ditokenisasi dan ditambahkan ke corpus\n",
        "\n",
        "    # Posisi paling bawah (yaitu posisi -1)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)  # Token dari pertanyaan user di-vektorisasi\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)  # Hitung similarity setiap token corpus dengan token pertanyaan\n",
        "\n",
        "    # Sortir jarak similarity setiap token corpus dengan token pertanyaan\n",
        "    # Mengambil indeks token yang memiliki similarity tertinggi (indeks paling rendah) dengan pertanyaan pengguna\n",
        "    idx = vals.argsort()[0][-2]\n",
        "\n",
        "    # Meratakan dan mengurutkan nilai similarity untuk mendapatkan similarity tertinggi kedua\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "\n",
        "    # Menyimpan nilai similarity tertinggi kedua\n",
        "    req_tfidf = flat[-2]\n",
        "\n",
        "    # Jika pertanyaan dan semua token corpus jaraknya tinggi, maka pertanyaan tidak memiliki jawaban\n",
        "    if req_tfidf == 0:\n",
        "        robo_response = robo_response + \"Please reply, I don't understand your questions\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        # Jika jaraknya terendah, maka token tersebut dipakai sebagai jawaban\n",
        "        robo_response = robo_response + sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "metadata": {
        "id": "oSBLT9vMLLUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge77U0ecmGxm"
      },
      "source": [
        "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon userʼs input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9dH6SSQmGxm",
        "outputId": "4f60235a-f32c-4250-87d2-dd82d1f432fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesin: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
            "Masukkan pertanyaan :hi\n",
            "Mesin: hi there\n",
            "Masukkan pertanyaan :What are the categories of chatbot usage?\n",
            "Mesin: analytics\n",
            "the usage of the chatbot can be monitored in order to spot potential flaws or problems.\n",
            "------------------------------------------\n",
            "Masukkan pertanyaan :Who coined the term \"ChatterBot\" and when?\n",
            "Mesin: the term \"chatterbot\" was originally coined by michael mauldin (creator of the first verbot, julia) in 1994 to describe these conversational programs.today, most chatbots are either accessed via virtual assistants such as google assistant and amazon alexa, via messaging apps such as facebook messenger or wechat, or via individual organizations' apps and websites.\n",
            "------------------------------------------\n",
            "Masukkan pertanyaan :What is the difference between older chatbots like ELIZA and PARRY compared to newer generations?\n",
            "Mesin: while eliza and parry were used exclusively to simulate typed conversation, many chatbots now include functional features such as games and web searching abilities.\n",
            "------------------------------------------\n",
            "Masukkan pertanyaan :How is the chatbot development process structured, and what are its key phases?\n",
            "Mesin: chatbot creation\n",
            "the process of creating a chatbot follows a pattern similar to the development of a web page or a mobile app.\n",
            "------------------------------------------\n",
            "Masukkan pertanyaan :What is the impact of artificial intelligence (AI) usage on jobs, according to Forrester?\n",
            "Mesin: according to forrester (2015), ai will replace 16 percent of american jobs by the end of the decade.chatbots have been used in applications such as customer service, sales and product education.\n",
            "------------------------------------------\n",
            "Masukkan pertanyaan :thank you\n",
            "Mesin: You are welcome..\n"
          ]
        }
      ],
      "source": [
        "# Inisialisasi variabel 'flag' untuk menjalankan loop\n",
        "flag=True\n",
        "print(\"Mesin: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "\n",
        "# Memulai loop utama, akan terus berjalan selama 'flag' bernilai True\n",
        "while(flag==True):\n",
        "    # Meminta user untuk memasukkan pertanyaan\n",
        "    user_response = input(\"Masukkan pertanyaan :\")\n",
        "    # Mengubah input pengguna menjadi huruf kecil untuk konsistensi\n",
        "    user_response=user_response.lower()\n",
        "\n",
        "    # Memeriksa jika user tidak keluar\n",
        "    if(user_response!='bye'):\n",
        "        # Jika pengguna mengucapkan thanks/thankyou, menghentikan loop & memberikan balasan\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False #tandai proses berhenti\n",
        "            print(\"Mesin: You are welcome..\") #balasan thank you\n",
        "        else:\n",
        "            if(greeting(user_response)!=None): #jika response adalah kalimat greeting\n",
        "                print(\"Mesin: \"+greeting(user_response)) #tampilkan kalimat greeting\n",
        "            else: #jika bukan kalimat greeting\n",
        "                print(\"Mesin: \",end=\"\")\n",
        "                print(response(user_response)) #memproses user answer disini\n",
        "                sent_tokens.remove(user_response) #user answer dihapus setelah dimunculkan\n",
        "                print(\"------------------------------------------\")\n",
        "\n",
        "    # Jika pengguna ingin keluar, mengubah nilai flag menjadi False untuk menghentikan loop\n",
        "    else:\n",
        "        flag=False\n",
        "        # Menampilkan pesan perpisahan dan pemisah untuk setiap iterasi\n",
        "        print(\"Mesin: Bye! take care..\")\n",
        "        print(\"========================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KESIMPULAN**\n",
        "\n",
        "- Berdasarkan output di atas, fungsi ***cosine similarity*** dalam program berjalan dengan baik. Ketika pengguna memasukkan pertanyaan, program menghitung kemiripan cosinus antara vektor TF-IDF dari pertanyaan tersebut dengan vektor TF-IDF dari seluruh korpus (pertanyaan sebelumnya). Hasil similarity digunakan untuk menentukan jawaban terbaik dengan memilih pertanyaan sebelumnya yang paling mirip.\n",
        "\n",
        "- Contohnya, ketika pengguna memasukkan pertanyaan \"What are the categories of chatbot usage?\" program memberikan jawaban yang relevan dengan mencocokkan kata kunci dan struktur pertanyaan dengan pertanyaan yang ada dalam korpus. Ini menunjukkan bahwa cosine similarity berfungsi dengan baik dalam memahami konteks pertanyaan dan memberikan respons yang sesuai."
      ],
      "metadata": {
        "id": "6morZ2S8SBd2"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}